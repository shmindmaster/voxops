{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6b4e04",
   "metadata": {},
   "source": [
    "## **Building Your Voice-to-Voice Agent with Azure AI Speech and AOAI**\n",
    "\n",
    "This notebook provides a step-by-step guide to create a voice-to-voice agent using Azure AI Speech services and Azure OpenAI. It walks you through the process of configuring speech recognition, integrating external tools, and generating human-like responses for real-time interactions.\n",
    "\n",
    "![alt text](../utils/images/lab1.png) \n",
    "\n",
    "\n",
    "1. **Audio Ingestion**: Ensure the capability to record audio is set up.  \n",
    "2. **Azure Speech-to-Text (STT)**: Converts live audio into transcribed text for LLM processing.  \n",
    "3. **Azure OpenAI with Function Calling & Streaming**: Understands patient intent, routes queries, and dynamically calls backend tools in real time.  \n",
    "4. **Azure Text-to-Speech (TTS)**: Delivers natural, empathetic voice responses back to the user in chunks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed102ccb",
   "metadata": {},
   "source": [
    "## **Audio Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71801339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available audio devices:\n",
      "0: Microsoft Sound Mapper - Input\n",
      "1: Surface Stereo Microphones (Sur\n",
      "2: Microsoft Sound Mapper - Output\n",
      "3: Surface Omnisonic Speakers (Sur\n",
      "4: Speakers (Dell USB Audio)\n",
      "5: Primary Sound Capture Driver\n",
      "6: Surface Stereo Microphones (Surface High Definition Audio)\n",
      "7: Primary Sound Driver\n",
      "8: Surface Omnisonic Speakers (Surface High Definition Audio)\n",
      "9: Speakers (Dell USB Audio)\n",
      "10: Speakers (Dell USB Audio)\n",
      "11: Surface Omnisonic Speakers (Surface High Definition Audio)\n",
      "12: Surface Stereo Microphones (Surface High Definition Audio)\n",
      "13: Headphones ()\n",
      "14: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva‚Äôs AirPods Pro #2))\n",
      "15: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva‚Äôs AirPods Pro #2))\n",
      "16: Speakers (Dell USB Audio)\n",
      "17: Microphone (Dell USB Audio)\n",
      "18: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva‚Äôs AirPods Pro #2 - Find My))\n",
      "19: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Shiva‚Äôs AirPods Pro #2 - Find My))\n",
      "20: Headphones 1 (Realtek HD Audio 2nd output with SST)\n",
      "21: Headphones 2 (Realtek HD Audio 2nd output with SST)\n",
      "22: PC Speaker (Realtek HD Audio 2nd output with SST)\n",
      "23: Speakers 1 (Realtek HD Audio output with SST)\n",
      "24: Speakers 2 (Realtek HD Audio output with SST)\n",
      "25: PC Speaker (Realtek HD Audio output with SST)\n",
      "26: Microphone Array (Realtek HD Audio Mic input)\n",
      "27: Headset Microphone (Headset Microphone)\n",
      "28: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods #3))\n",
      "29: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods #3))\n",
      "30: Input ()\n",
      "31: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods #4))\n",
      "32: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods #4))\n",
      "33: Headphones ()\n",
      "34: Output (@System32\\drivers\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\n",
      ";(iPhone de Pablo))\n",
      "35: Input (@System32\\drivers\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\n",
      ";(iPhone de Pablo))\n",
      "36: Headphones ()\n",
      "37: Headphones ()\n",
      "38: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods Pro - Find My))\n",
      "39: Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Pablo‚Äôs AirPods Pro - Find My))\n",
      "40: Headphones ()\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "\n",
    "def list_audio_devices():\n",
    "    \"\"\"\n",
    "    List all available audio devices using PyAudio.\n",
    "\n",
    "    This function initializes PyAudio, retrieves the list of audio devices,\n",
    "    and prints their names. It also includes error handling to ensure proper\n",
    "    cleanup of resources.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = pyaudio.PyAudio()\n",
    "        print(\"Available audio devices:\")\n",
    "        for ii in range(p.get_device_count()):\n",
    "            device_name = p.get_device_info_by_index(ii).get(\"name\")\n",
    "            print(f\"{ii}: {device_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while listing audio devices: {e}\")\n",
    "    finally:\n",
    "        # Ensure PyAudio resources are released\n",
    "        if \"p\" in locals():\n",
    "            p.terminate()\n",
    "\n",
    "\n",
    "# Call the function to list audio devices\n",
    "list_audio_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9ade96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete. Saving audio...\n",
      "Audio saved to test_audio.wav. Playing back...\n",
      "Playback complete.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "\n",
    "def test_microphone():\n",
    "    \"\"\"\n",
    "    Test the microphone by recording audio and playing it back.\n",
    "\n",
    "    This function captures audio from the default input device (microphone),\n",
    "    saves it to a temporary WAV file, and plays it back to ensure the microphone\n",
    "    is working correctly.\n",
    "    \"\"\"\n",
    "    # Audio configuration\n",
    "    chunk = 1024  # Number of frames per buffer\n",
    "    format = pyaudio.paInt16  # 16-bit audio format\n",
    "    channels = 1  # Mono audio\n",
    "    rate = 44100  # Sampling rate (44.1 kHz)\n",
    "    record_seconds = 5  # Duration of the recording\n",
    "    output_filename = \"test_audio.wav\"\n",
    "\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    try:\n",
    "        # Open the microphone stream\n",
    "        print(\"Recording...\")\n",
    "        stream = p.open(\n",
    "            format=format,\n",
    "            channels=channels,\n",
    "            rate=rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=chunk,\n",
    "        )\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        # Record audio in chunks\n",
    "        for _ in range(0, int(rate / chunk * record_seconds)):\n",
    "            data = stream.read(chunk)\n",
    "            frames.append(data)\n",
    "\n",
    "        print(\"Recording complete. Saving audio...\")\n",
    "\n",
    "        # Save the recorded audio to a WAV file\n",
    "        with wave.open(output_filename, \"wb\") as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(p.get_sample_size(format))\n",
    "            wf.setframerate(rate)\n",
    "            wf.writeframes(b\"\".join(frames))\n",
    "\n",
    "        print(f\"Audio saved to {output_filename}. Playing back...\")\n",
    "\n",
    "        # Play back the recorded audio\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "        # Open the WAV file for playback\n",
    "        wf = wave.open(output_filename, \"rb\")\n",
    "        playback_stream = p.open(\n",
    "            format=p.get_format_from_width(wf.getsampwidth()),\n",
    "            channels=wf.getnchannels(),\n",
    "            rate=wf.getframerate(),\n",
    "            output=True,\n",
    "        )\n",
    "\n",
    "        # Read and play audio data\n",
    "        data = wf.readframes(chunk)\n",
    "        while data:\n",
    "            playback_stream.write(data)\n",
    "            data = wf.readframes(chunk)\n",
    "\n",
    "        playback_stream.stop_stream()\n",
    "        playback_stream.close()\n",
    "\n",
    "        print(\"Playback complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Terminate PyAudio\n",
    "        p.terminate()\n",
    "\n",
    "\n",
    "# Run the microphone test\n",
    "test_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9da16",
   "metadata": {},
   "source": [
    "## **Define Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f52ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'audioagent (Python 3.11.11)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = (\n",
    "    r\"C:\\Users\\pablosal\\Desktop\\gbb-ai-audio-agent\"  # change your directory here\n",
    ")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a11c15",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'audioagent (Python 3.11.11)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from src.speech.speech_recognizer import SpeechRecognizer, StreamingSpeechRecognizer\n",
    "from src.speech.text_to_speech import SpeechSynthesizer\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Ensure clients are initialized only if not already defined\n",
    "if \"az_speech_recognizer_client\" not in locals():\n",
    "    az_speech_recognizer_client = SpeechRecognizer()\n",
    "\n",
    "if \"az_speech_recognizer_stream_client\" not in locals():\n",
    "    az_speech_recognizer_stream_client = StreamingSpeechRecognizer(\n",
    "        vad_silence_timeout_ms=5000\n",
    "    )\n",
    "\n",
    "if \"az_speach_synthesizer_client\" not in locals():\n",
    "    az_speach_synthesizer_client = SpeechSynthesizer()\n",
    "\n",
    "# Ensure Azure OpenAI client is initialized only if not already defined\n",
    "if \"client\" not in locals():\n",
    "    client = AzureOpenAI(\n",
    "        api_version=\"2025-02-01-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ee563",
   "metadata": {},
   "source": [
    "## **Azure Speech-to-Text (STT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96a82c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 20:27:10,910 - micro - MainProcess - INFO     Starting continuous recognition ‚Ä¶ (speech_recognizer.py:start:134)\n",
      "2025-06-16 20:27:10,948 - micro - MainProcess - INFO     Recognition started with languages=['en-US', 'es-ES', 'fr-FR'] (fallback=en-US) (speech_recognizer.py:start:171)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial (en-US): can you please\n",
      "Partial (en-US): can you please describe\n",
      "Partial (en-US): can you please describe madrid\n",
      "Partial (en-US): can you please describe madrid in\n",
      "Partial (en-US): can you please describe madrid in 100 words\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian then\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian then to\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian then to english\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian then to english and then\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian then to english and then finish with\n",
      "Partial (en-US): can you please describe madrid in 100 words you need to start first in spanish then switch to italian then to english and then finish with french\n",
      "Final (en-US): Can you please describe Madrid in 100 words? You need to start first in Spanish, then switch to Italian, then to English and then finish with French.\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "recognizer = StreamingSpeechRecognizer(vad_silence_timeout_ms=3000)\n",
    "\n",
    "# Buffers\n",
    "all_text_live = \"\"\n",
    "final_transcripts = []\n",
    "\n",
    "# Internal tracker to prevent double-processing\n",
    "last_final_text = None\n",
    "\n",
    "\n",
    "def on_partial(text: str, lang: str):\n",
    "    global all_text_live\n",
    "    print(f\"Partial ({lang}): {text}\")\n",
    "    all_text_live = text\n",
    "\n",
    "\n",
    "def on_final(text: str, lang: str):\n",
    "    global all_text_live, last_final_text\n",
    "    if text == last_final_text:\n",
    "        return\n",
    "    last_final_text = text\n",
    "\n",
    "    print(f\"Final ({lang}): {text}\")\n",
    "    final_transcripts.append(text)\n",
    "    all_text_live = \"\"\n",
    "\n",
    "\n",
    "# Attach\n",
    "recognizer.set_partial_result_callback(on_partial)\n",
    "recognizer.set_final_result_callback(on_final)\n",
    "\n",
    "# Start recognizing\n",
    "recognizer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ebc8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you please describe Madrid in 100 words? You need to start first in Spanish, then switch to Italian, then to English and then finish with French. '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_conversation = \" \".join(final_transcripts) + \" \" + all_text_live\n",
    "full_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "698b9a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 20:27:30,654 - micro - MainProcess - INFO     Stopping recognition ‚Ä¶ (speech_recognizer.py:stop:180)\n",
      "2025-06-16 20:27:30,736 - micro - MainProcess - INFO     Session stopped: SessionEventArgs(session_id=3ba6dd0a566543a69adeae71014e6941) (speech_recognizer.py:_on_session_stopped:222)\n",
      "2025-06-16 20:27:30,739 - micro - MainProcess - INFO     Recognition stopped. (speech_recognizer.py:stop:182)\n"
     ]
    }
   ],
   "source": [
    "# Start recognizing\n",
    "recognizer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96eabf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 20:26:02,124 - micro - MainProcess - INFO     Starting continuous recognition ‚Ä¶ (speech_recognizer.py:start:134)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting microphone recognition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 20:26:02,511 - micro - MainProcess - INFO     Recognition started with languages=['en-US', 'es-ES', 'fr-FR'] (fallback=en-US) (speech_recognizer.py:start:171)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Listening... (speak now)\n",
      "Partial (en-US): can you please describe\n",
      "Partial (en-US): can you please describe madrid\n",
      "Partial (en-US): can you please describe madrid in\n",
      "Partial (en-US): can you please describe madrid in 100\n",
      "Partial (en-US): can you please describe madrid in 100 words\n",
      "Partial (en-US): can you please describe madrid in 100 words in\n",
      "Partial (en-US): can you please describe madrid in 100 words in half\n",
      "Partial (en-US): can you please describe madrid in 100 words in half english\n",
      "Partial (en-US): can you please describe madrid in 100 words in half english half\n",
      "Partial (en-US): can you please describe madrid in 100 words in half english half spanish\n",
      "Partial (en-US): can you please describe madrid in 100 words in half english half spanish half\n",
      "Partial (en-US): can you please describe madrid in 100 words in half english half spanish half italian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 20:26:15,375 - micro - MainProcess - INFO     Stopping recognition ‚Ä¶ (speech_recognizer.py:stop:180)\n",
      "2025-06-16 20:26:15,450 - micro - MainProcess - INFO     Session stopped: SessionEventArgs(session_id=2822193b3f5644c796f485ae5953742d) (speech_recognizer.py:_on_session_stopped:222)\n",
      "2025-06-16 20:26:15,503 - micro - MainProcess - INFO     Recognition stopped. (speech_recognizer.py:stop:182)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final (en-US): Can you please describe Madrid in 100 words in half English, half Spanish, half Italian?\n",
      "üõë Recognition stopped.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "SILENCE_THRESHOLD = 20  # seconds of silence to stop recognition\n",
    "\n",
    "\n",
    "def handle_speech_recognition() -> str:\n",
    "    global all_text_live, last_final_text\n",
    "\n",
    "    final_transcripts = []\n",
    "\n",
    "    print(\"Starting microphone recognition...\")\n",
    "    final_transcripts.clear()\n",
    "    all_text_live = \"\"\n",
    "    last_final_text = None\n",
    "\n",
    "    def on_partial(text: str, lang: str):\n",
    "        global all_text_live\n",
    "        print(f\"Partial ({lang}): {text}\")\n",
    "        all_text_live = text\n",
    "\n",
    "    def on_final(text: str, lang: str):\n",
    "        global all_text_live, last_final_text\n",
    "        if text == last_final_text:\n",
    "            return\n",
    "        last_final_text = text\n",
    "\n",
    "        print(f\"Final ({lang}): {text}\")\n",
    "        final_transcripts.append(text)\n",
    "        all_text_live = \"\"\n",
    "\n",
    "    recognizer.set_partial_result_callback(on_partial)\n",
    "    recognizer.set_final_result_callback(on_final)\n",
    "\n",
    "    recognizer.start()\n",
    "    print(\"üé§ Listening... (speak now)\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    while not final_transcripts and (time.time() - start_time < SILENCE_THRESHOLD):\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    recognizer.stop()\n",
    "    print(\"üõë Recognition stopped.\")\n",
    "\n",
    "    return \" \".join(final_transcripts) + \" \" + all_text_live\n",
    "\n",
    "\n",
    "recognition = handle_speech_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21400ef9",
   "metadata": {},
   "source": [
    "## **Azure OpenAI Streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d92bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro, aqu√≠ tienes una definici√≥n del concepto de inteligencia artificial en coreano:\n",
      "\n",
      "Ïù∏Í≥µÏßÄÎä•(AI)ÏùÄ Ïª¥Ìì®ÌÑ∞ ÏãúÏä§ÌÖúÏù¥ Ïù∏Í∞ÑÏùò ÏßÄÎä•ÏùÑ Î™®Î∞©ÌïòÏó¨ ÏûëÏóÖÏùÑ ÏàòÌñâÌïòÎäî Í∏∞Ïà†ÏûÖÎãàÎã§. Ïù¥Îäî ÌïôÏäµ, Ï∂îÎ°†, Î¨∏Ï†ú Ìï¥Í≤∞, Ïù¥Ìï¥ Î∞è ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨ÏôÄ Í∞ôÏùÄ Îä•Î†•ÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§. Ïù∏Í≥µÏßÄÎä•ÏùÄ Î®∏Ïã†Îü¨Îãù, Îî•Îü¨ÎãùÍ≥º Í∞ôÏùÄ ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÏÑùÌïòÍ≥† Ìå®ÌÑ¥ÏùÑ Ïù∏ÏãùÌï©ÎãàÎã§. Ïù¥Î•º ÌÜµÌï¥ ÏûêÏú®Ï†Å ÏùòÏÇ¨Í≤∞Ï†ïÏù¥ Í∞ÄÎä•Ìï¥Ï†∏ Îã§ÏñëÌïú Î∂ÑÏïºÏóêÏÑú ÌòÅÏã†ÏùÑ Ïù¥Î£®Í≥† ÏûàÏäµÎãàÎã§. ÏùòÎ£å, Í∏àÏúµ, ÏûêÎèôÏ∞®, Î°úÎ¥á Í≥µÌïô Îì±Ïùò ÏÇ∞ÏóÖ Î∂ÑÏïºÏóêÏÑú Ïù∏Í≥µÏßÄÎä•ÏùÄ Ìö®Ïú®ÏÑ±ÏùÑ ÎÜíÏù¥Í≥† ÏÉàÎ°úÏö¥ Í∞ÄÎä•ÏÑ±ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§. Ïù∏Í≥µÏßÄÎä•ÏùÄ Ïù∏Í∞ÑÏùò ÏÇ∂ÏùÑ Î≥ÄÌôîÏãúÌÇ¨ Ïàò ÏûàÎäî Ïû†Ïû¨Î†•ÏùÑ ÏßÄÎãàÍ≥† ÏûàÏäµÎãàÎã§."
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"puedes definir el concepto de inteligencia artificial en korean en 100 palabras?\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")\n",
    "full_response = \"\"\n",
    "for update in response:\n",
    "    if update.choices:\n",
    "        chunk = update.choices[0].delta.content or \"\"\n",
    "        full_response += chunk\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486daef",
   "metadata": {},
   "source": [
    "## **Azure Text-to-Speech (TTS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82328088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "az_speach_synthesizer_client.start_speaking_text(\n",
    "    text=full_response,\n",
    ")\n",
    "time.sleep(2)\n",
    "az_speach_synthesizer_client.stop_speaking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c6a37",
   "metadata": {},
   "source": [
    "## **Streaming Audio Back to the User**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d764bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_sentence_end = [\".\", \"!\", \"?\", \";\", \"„ÄÇ\", \"ÔºÅ\", \"Ôºü\", \"Ôºõ\", \"\\n\"]\n",
    "completion = client.chat.completions.create(\n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{full_conversation}\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")\n",
    "\n",
    "collected_messages = []\n",
    "last_tts_request = None\n",
    "\n",
    "for chunk in completion:\n",
    "    if len(chunk.choices) > 0:\n",
    "        chunk_text = chunk.choices[0].delta.content\n",
    "        if chunk_text:\n",
    "            collected_messages.append(chunk_text)\n",
    "            if chunk_text in tts_sentence_end:\n",
    "                text = \"\".join(collected_messages).strip()\n",
    "                last_tts_request = az_speach_synthesizer_client.start_speaking_text(\n",
    "                    text\n",
    "                )\n",
    "                collected_messages.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffdc7b",
   "metadata": {},
   "source": [
    "## **Adding Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a19e4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Templates found: ['voice_agent_system.jinja', 'voice_agent_user.jinja']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "from app.backend.prompt_manager import PromptManager\n",
    "\n",
    "prompt_manager = PromptManager()\n",
    "systemp_prompt = prompt_manager.get_prompt(\"voice_agent_system.jinja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c217c",
   "metadata": {},
   "source": [
    "## **Adding Tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e66946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.backend.tools import available_tools\n",
    "from app.backend.functions import (\n",
    "    schedule_appointment,\n",
    "    refill_prescription,\n",
    "    lookup_medication_info,\n",
    "    evaluate_prior_authorization,\n",
    "    escalate_emergency,\n",
    "    authenticate_user,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0da14961",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "        \"this is an emergency. I need to schedule an appointment with my doctor ASAP.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f201ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping tool names to actual Python async functions\n",
    "function_mapping = {\n",
    "    \"schedule_appointment\": schedule_appointment,\n",
    "    \"refill_prescription\": refill_prescription,\n",
    "    \"lookup_medication_info\": lookup_medication_info,\n",
    "    \"evaluate_prior_authorization\": evaluate_prior_authorization,\n",
    "    \"escalate_emergency\": escalate_emergency,\n",
    "    \"authenticate_user\": authenticate_user,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2716c04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_mIxsi3OwoVbUtqSgjTxDFtTK', function=Function(arguments='{\"reason\":\"Need to schedule an appointment urgently.\"}', name='escalate_emergency'), type='function')])\n",
      "Function result: üö® Emergency escalation triggered: Need to schedule an appointment urgently.. A human healthcare agent is now being connected.\n",
      "Tool Name: escalate_emergency\n",
      "Final response:\n",
      "I‚Äôve escalated your request to a human healthcare agent who will assist you with scheduling your appointment immediately. Please hold on for a moment.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=conversation_history,\n",
    "    tools=available_tools,\n",
    "    tool_choice=\"auto\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.5,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")\n",
    "\n",
    "# Process the model's response\n",
    "response_message = response.choices[0].message\n",
    "conversation_history.append(response_message)\n",
    "\n",
    "print(\"Model's response:\")\n",
    "print(response_message)\n",
    "\n",
    "if response_message.tool_calls:\n",
    "    for tool_call in response_message.tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "        # Check if the function name is in the mapping\n",
    "        if function_name in function_mapping:\n",
    "            # Call the corresponding Python async function\n",
    "            result = await function_mapping[function_name](**function_args)\n",
    "            print(f\"Function result: {result}\")\n",
    "            print(f\"Tool Name: {function_name}\")\n",
    "            conversation_history.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": result,\n",
    "                }\n",
    "            )\n",
    "else:\n",
    "    print(\"No tool calls were made by the model.\")\n",
    "\n",
    "# Second API call: Get the final response from the model\n",
    "final_response = client.chat.completions.create(\n",
    "    messages=conversation_history,\n",
    "    max_tokens=4096,\n",
    "    temperature=0.5,\n",
    "    top_p=1.0,\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    ")\n",
    "\n",
    "print(\"Final response:\")\n",
    "print(final_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70a5c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4fbffb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm: 13>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speechsdk.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734714e0",
   "metadata": {},
   "source": [
    "## **Making Tools work in Streaming Fashion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfe00db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "        \"this is an emergency. I need to schedule an appointment with my doctor ASAP.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f55da9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_VWhpt20nqoslFiup9FVvb9Dd', function=ChoiceDeltaToolCallFunction(arguments='', name='escalate_emergency'), type='function')])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='reason', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='Need', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' to', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' schedule', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' an', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' emergency', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' appointment', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' with', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' doctor', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' ASAP', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='.\"', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='}', name=None), type=None)])\n",
      "delta: ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "tool_name:escalate_emergency\n",
      "tool_id:call_VWhpt20nqoslFiup9FVvb9Dd\n",
      "tool_call_accumulator:{\"reason\":\"Need to schedule an emergency appointment with doctor ASAP.\"}\n",
      "Function result: üö® Emergency escalation triggered: Request for immediate appointment scheduling due to emergency.. A human healthcare agent is now being connected.\n",
      "=============================================\n",
      "I've triggered an emergency escalation for your request, and a human healthcare agent is now being connected to assist you. Please hold on for a moment while they join the conversation. Your safety is our priority."
     ]
    }
   ],
   "source": [
    "# initial user message\n",
    "tool_call_accumulator = \"\"\n",
    "\n",
    "# First API call: Ask the model to use the function\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    messages=conversation_history,\n",
    "    tools=available_tools,\n",
    "    tool_choice=\"auto\",\n",
    "    stream=True,  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "# process the model\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "        print(f\"delta: {delta}\")  # print delta from the chunk for learning\n",
    "\n",
    "        if delta.tool_calls:\n",
    "            if delta.tool_calls[0].function.name:\n",
    "                tool_name = chunk.choices[0].delta.tool_calls[0].function.name\n",
    "                tool_id = chunk.choices[0].delta.tool_calls[0].id\n",
    "                conversation_history.append(delta)\n",
    "\n",
    "            if chunk.choices[0].delta.tool_calls[0].function.arguments:\n",
    "                tool_call_accumulator += delta.tool_calls[0].function.arguments\n",
    "\n",
    "# print function related outputs\n",
    "print(f\"tool_name:{tool_name}\")\n",
    "print(f\"tool_id:{tool_id}\")\n",
    "print(f\"tool_call_accumulator:{tool_call_accumulator}\")\n",
    "# Check if the function name is in the mapping\n",
    "if tool_name in function_mapping:\n",
    "    # Call the corresponding Python async function\n",
    "    result = await function_mapping[function_name](**function_args)\n",
    "    print(f\"Function result: {result}\")\n",
    "    conversation_history.append(\n",
    "        {\n",
    "            \"tool_call_id\": tool_id,\n",
    "            \"role\": \"tool\",\n",
    "            \"name\": tool_name,\n",
    "            \"content\": result,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# print(f\"messages: {messages}\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "#  second API call: Get the final response with stream\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    messages=conversation_history,\n",
    "    temperature=0.7,\n",
    "    stream=True,  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if len(chunk.choices) > 0:\n",
    "        delta = chunk.choices[0].delta\n",
    "        # print(f\"delta: {delta}\")\n",
    "        if delta.content:\n",
    "            print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636a8d5",
   "metadata": {},
   "source": [
    "## **Run the Conversation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0de55282",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello, my name is Pablo Salvador. \"\n",
    "        \"this is an emergency. I need to schedule an appointment with my doctor ASAP.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f5c9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history: List[Dict[str, str]] = [\n",
    "    {\"role\": \"system\", \"content\": systemp_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, my name is Pablo Salvador. \"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea4f10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "tts_sentence_end = [\".\", \"!\", \"?\", \";\", \"„ÄÇ\", \"ÔºÅ\", \"Ôºü\", \"Ôºõ\", \"\\n\"]\n",
    "\n",
    "\n",
    "async def handle_chat(conversation_history: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Handles a full streaming chat round with Azure OpenAI GPT-4o,\n",
    "    correctly executes tool calls, synthesizes responses, and continues reasoning.\n",
    "    \"\"\"\n",
    "    tool_name = None\n",
    "    function_call_arguments = \"\"\n",
    "    tool_call_id = None\n",
    "    last_tts_request = None\n",
    "    collected_messages: List[str] = []\n",
    "\n",
    "    # üîÅ FIRST STREAMING RESPONSE (may include tool call)\n",
    "    response = client.chat.completions.create(\n",
    "        stream=True,\n",
    "        messages=conversation_history,\n",
    "        tools=available_tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096,\n",
    "        temperature=0.5,\n",
    "        top_p=1.0,\n",
    "        model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "    )\n",
    "\n",
    "    for chunk in response:\n",
    "        if chunk.choices:\n",
    "            delta = chunk.choices[0].delta\n",
    "\n",
    "            if delta.tool_calls:\n",
    "                if delta.tool_calls[0].function.name:\n",
    "                    tool_name = chunk.choices[0].delta.tool_calls[0].function.name\n",
    "                    tool_id = chunk.choices[0].delta.tool_calls[0].id\n",
    "                    conversation_history.append(delta)\n",
    "\n",
    "                if chunk.choices[0].delta.tool_calls[0].function.arguments:\n",
    "                    function_call_arguments += delta.tool_calls[0].function.arguments\n",
    "\n",
    "            elif delta.content:\n",
    "                chunk_text = chunk.choices[0].delta.content\n",
    "                if chunk_text:\n",
    "                    collected_messages.append(chunk_text)\n",
    "                    if chunk_text in tts_sentence_end:\n",
    "                        text = \"\".join(collected_messages).strip()\n",
    "                        last_tts_request = (\n",
    "                            az_speach_synthesizer_client.start_speaking_text(text)\n",
    "                        )\n",
    "                        collected_messages.clear()\n",
    "\n",
    "    # üß† If tool call was detected, execute it\n",
    "    if tool_name:\n",
    "        print(f\"tool_name:{tool_name}\")\n",
    "        print(f\"tool_id:{tool_id}\")\n",
    "        print(f\"function_call_arguments:{function_call_arguments}\")\n",
    "        try:\n",
    "            parsed_args = json.loads(function_call_arguments.strip())\n",
    "            function_to_call = function_mapping.get(tool_name)\n",
    "\n",
    "            if function_to_call:\n",
    "                result = await function_to_call(parsed_args)\n",
    "\n",
    "                print(f\"‚úÖ Function `{tool_name}` executed. Result: {result}\")\n",
    "\n",
    "                conversation_history.append(\n",
    "                    {\n",
    "                        \"tool_call_id\": tool_id,\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": result,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # üß† SECOND STREAMING CALL AFTER TOOL EXECUTION\n",
    "                second_response = client.chat.completions.create(\n",
    "                    stream=True,\n",
    "                    messages=conversation_history,\n",
    "                    temperature=0.5,\n",
    "                    top_p=1.0,\n",
    "                    max_tokens=4096,\n",
    "                    model=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_ID\"),\n",
    "                )\n",
    "\n",
    "                collected_messages = []\n",
    "\n",
    "                for chunk in second_response:\n",
    "                    if chunk.choices:\n",
    "                        delta = chunk.choices[0].delta\n",
    "                        if hasattr(delta, \"content\") and delta.content:\n",
    "                            chunk_message = delta.content\n",
    "                            collected_messages.append(chunk_message)\n",
    "                            if chunk_message.strip() in tts_sentence_end:\n",
    "                                text = \"\".join(collected_messages).strip()\n",
    "                                if text:\n",
    "                                    az_speach_synthesizer_client.start_speaking_text(\n",
    "                                        text\n",
    "                                    )\n",
    "                                    collected_messages.clear()\n",
    "\n",
    "                final_text = \"\".join(collected_messages).strip()\n",
    "                if final_text:\n",
    "                    conversation_history.append(\n",
    "                        {\"role\": \"assistant\", \"content\": final_text}\n",
    "                    )\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ùå Error parsing function arguments: {e}\")\n",
    "\n",
    "    else:\n",
    "        # Append the assistant message if no function call was made\n",
    "        final_text = \"\".join(collected_messages).strip()\n",
    "        if final_text:\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": final_text})\n",
    "            print(f\"‚úÖ Final assistant message: {final_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6111fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_name:escalate_emergency\n",
      "tool_id:call_hbMdkWeNNrbjDUEhUih6vQEK\n",
      "function_call_arguments:{\"reason\":\"urgent appointment needed\"}\n",
      "‚úÖ Function `escalate_emergency` executed. Result: üö® Emergency escalation triggered: {'reason': 'urgent appointment needed'}. A human healthcare agent is now being connected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 00:42:22,451 - micro - MainProcess - INFO     [üîä] Starting streaming speech synthesis for text: I understand that this is an e... (text_to_speech.py:start_speaking_text:44)\n",
      "INFO:micro:[üîä] Starting streaming speech synthesis for text: I understand that this is an e...\n",
      "2025-04-15 00:42:22,529 - micro - MainProcess - INFO     [üîä] Starting streaming speech synthesis for text: I've escalated your request, a... (text_to_speech.py:start_speaking_text:44)\n",
      "INFO:micro:[üîä] Starting streaming speech synthesis for text: I've escalated your request, a...\n",
      "2025-04-15 00:42:22,535 - micro - MainProcess - INFO     [üîä] Starting streaming speech synthesis for text: Please hold on for a moment.... (text_to_speech.py:start_speaking_text:44)\n",
      "INFO:micro:[üîä] Starting streaming speech synthesis for text: Please hold on for a moment....\n"
     ]
    }
   ],
   "source": [
    "await handle_chat(conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7cc26a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 17:05:05,507 - micro - MainProcess - INFO     [üõë] Stopping speech synthesis... (text_to_speech.py:stop_speaking:55)\n",
      "INFO:micro:[üõë] Stopping speech synthesis...\n"
     ]
    }
   ],
   "source": [
    "az_speach_synthesizer_client.stop_speaking()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
